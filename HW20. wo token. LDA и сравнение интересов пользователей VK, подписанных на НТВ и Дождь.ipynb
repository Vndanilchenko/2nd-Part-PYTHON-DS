{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3\n",
    "## Сравнение интересов аудитории телеканалов НТВ и Дождь с помощью тематического моделирования LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Сравнить интересы аудитории телеканолов НТВ и Дождь с помощью методов тематического моделирования\n",
    "1. Получить данные по аудитории из социальной сети ВК\n",
    "2. Зарегистрировать приложение, получить app_id, access_token\n",
    "3. Скачать данные по пользователям в каждой из групп (id групп ВК даны ниже, tvrain_id, ntv_id)\n",
    "4. Взять небольшую выборку из каждой совокупности телезрителей(около 1000-2000 человек, т.к. 300k-400k слишком много), с которыми работать дальше\n",
    "5. Обучить LDA модель на их подписках\n",
    "6. По группам, на которые подписаны эти люди, полуичть ключевые слова групп, на которые они подписаны\n",
    "7. Получить распределение интересов людей для каждой совокупности, сравнить на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import sys  \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для использования VK API необходимо создать приложение в VK\n",
    "\n",
    "1. Создать приложение по адресу https://vk.com/apps?act=manage (кнопка \"создать приложение\")\n",
    "2. При создании указать название, описание (можно любые), категория  - прочее. Тип - standalone приложение\n",
    "3. В настройках получить **app_id**. App_id потребуется для получения access token\n",
    "4. Авторизовать пользователя (получить access token) можно по адресу: https://vk.com/dev/first_guide, в правилах нас интересует пункт 3 **Авторизация пользователя**\n",
    "5. После того, как ознакомитесь с авторизацией пользователя, скопируйте в адресную строку такой запрос https://oauth.vk.com/authorize?client_id=5490057&display=page&redirect_uri=https://oauth.vk.com/blank.html&scope=friends&response_type=token&v=5.52, где число **5490057** замените на число, которое получите для вашего **app_id**\n",
    "6. Нажмите Enter. Откроется окно с запросом прав. В нем отображаются название приложения, иконки прав доступа, и Ваши имя с фамилией. Нажмите «Разрешить». Вы попадете на новую страницу с предупреждением о том, что токен нельзя копировать и передавать третьим лицам. В адресной строке будет URL https://oauth.vk.com/blank.html, а после # Вы увидите дополнительные параметры — access_token, expires_in и user_id. Токен может выглядеть, например, так: 51eff86578a3bbbcb5c7043a122a69fd04dca057ac821dd7afd7c2d8e35b60172d45a26599c08034cc40a\n",
    "7. Токен — это Ваш ключ доступа. При выполнении определенных условий человек, получивший Ваш токен, может нанести существенный ущерб Вашим данным и данным других людей. Поэтому очень важно не передавать свой токен третьим лицам\n",
    "8. Поле expires_in содержит время жизни токена в секундах. 86400 секунд — это ровно сутки. Через сутки полученный токен перестанет действовать, для продолжения работы нужно будет получить новый по такому же алгоритму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own app id and respective tokens\n",
    "\n",
    "# скопируйте сюда ваши app_id и access_token, полученные по методу, описанному выше\n",
    "app_id = ..\n",
    "access_token = ..\n",
    "\n",
    "# id групп ВК Дождя и НТВ\n",
    "tvrain_id = 17568841\n",
    "ntv_id = 28658784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Павел Дуров\n"
     ]
    }
   ],
   "source": [
    "# проверка работы API и авторизации пользователя. Если возникает ошибка, то, возможно, access token необходимо обновить\n",
    "check_id = 1\n",
    "\n",
    "# api call and test\n",
    "def vk_get_response(method, parameters, token):\n",
    "    url = 'https://api.vk.com/method/' + method + '?' + parameters + '&access_token=' + token\n",
    "#     print url\n",
    "    return(requests.get(url).json())\n",
    "\n",
    "answer = vk_get_response(\n",
    "    'users.get', 'user_ids={0}&v=4.9&lang=ru'.format(check_id), access_token\n",
    ")['response']\n",
    "print(answer[0]['first_name'], answer[0]['last_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получение подписчиков телеканалов НТВ и Дождь в VK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим объекты, которые содержат всю информацию о подписчиках соответствующих групп (указанных в domains) и сохраним их на диск. Получим в итоге два файла - **ntv_subs** и **tvrain_subs** в формате **.pkl** - питоновский формат хранения данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset:  0\n",
      "Offset:  1000\n",
      "Offset:  2000\n",
      "Offset:  3000\n",
      "Offset:  4000\n",
      "Offset:  5000\n",
      "Offset:  0\n",
      "Offset:  1000\n",
      "Offset:  2000\n",
      "Offset:  3000\n",
      "Offset:  4000\n",
      "Offset:  5000\n",
      "Offset:  444000\n"
     ]
    }
   ],
   "source": [
    "domains = ['ntv', 'tvrain']\n",
    "\n",
    "\n",
    "for group_domain in domains:\n",
    "    offset = 0\n",
    "    group_id = group_domain\n",
    "    fields = \"\"\"sex,bdate,city,country,home_town,lists,domain,has_mobile,\n",
    "    contacts,connections,education,universities,followers_count,occupation,last_seen,relation\"\"\"\n",
    "    first_sample = vk_get_response(\n",
    "        'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "            group_id, offset, fields\n",
    "        ), token=access_token\n",
    "    )\n",
    "    community_count = first_sample['response']['count']\n",
    "    community_members = []\n",
    "    for i in range(community_count // 1000 + 1):\n",
    "        offset = i * 1000\n",
    "        try:\n",
    "            answer = vk_get_response(\n",
    "                'groups.getMembers', 'group_id={0}&offset={1}&fields={2}&v=4.9&lang=ru'.format(\n",
    "                    group_id, offset, fields), token=access_token\n",
    "            )\n",
    "            if offset<=5000 or offset>=444000:\n",
    "                print(\"Offset: \", offset)\n",
    "        except:\n",
    "            print(\"Offset: \", offset, \" Error\")\n",
    "        community_members += answer['response']['users']\n",
    "    save_obj(community_members, '{}_subs'.format(group_domain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv = load_obj('ntv_subs')\n",
    "community_tvrain = load_obj('tvrain_subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ntv_df = pd.DataFrame(community_ntv)\n",
    "community_tvrain_df = pd.DataFrame(community_tvrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала получим всех уникальных подписчиков НТВ и Дождя с помощью unique. Далее с помощью numpy.random необходимо выбрать небольшой sample (например, по 1000 из каждой группы) таких людей и объединить их вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntv_uids = community_ntv_df.uid.unique().tolist()\n",
    "tvrain_uids = community_tvrain_df.uid.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить общий список людей из двух выборок НТВ и Дождя, всего должно быть в итоге около 2000 человек\n",
    "import random\n",
    "ntv_uids_shot=random.sample(ntv_uids, 1000)\n",
    "tvrain_uids_shot=random.sample(tvrain_uids, 1000)\n",
    "uids = list(np.ravel([ntv_uids_shot, tvrain_uids_shot]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 profiles done\n",
      "100 profiles done\n",
      "200 profiles done\n",
      "300 profiles done\n",
      "400 profiles done\n",
      "500 profiles done\n",
      "600 profiles done\n",
      "700 profiles done\n",
      "800 profiles done\n",
      "900 profiles done\n",
      "1000 profiles done\n",
      "1100 profiles done\n",
      "1200 profiles done\n",
      "1300 profiles done\n",
      "1400 profiles done\n",
      "1500 profiles done\n",
      "1600 profiles done\n",
      "1700 profiles done\n",
      "1800 profiles done\n",
      "1900 profiles done\n"
     ]
    }
   ],
   "source": [
    "# получить подписки этих пользователей\n",
    "print_counter = 0\n",
    "final_data = []\n",
    "\n",
    "for uid in uids:\n",
    "    try:\n",
    "        user_subs = vk_get_response(\n",
    "            'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "        )\n",
    "        time.sleep(0.3)\n",
    "        final_data.append(user_subs)\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    if print_counter % 100 == 0:\n",
    "        print(\"{0} profiles done\".format(print_counter))\n",
    "    print_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 users\n",
      "Processed 100 users\n",
      "Processed 200 users\n",
      "Processed 300 users\n",
      "Processed 300 users\n",
      "Processed 400 users\n",
      "Processed 400 users\n",
      "Processed 500 users\n",
      "Processed 600 users\n",
      "Processed 700 users\n",
      "Processed 800 users\n",
      "Processed 800 users\n",
      "Processed 900 users\n",
      "Processed 900 users\n",
      "Processed 1000 users\n",
      "Processed 1100 users\n",
      "Processed 1200 users\n",
      "Processed 1200 users\n",
      "Processed 1300 users\n",
      "Processed 1400 users\n",
      "Processed 1400 users\n",
      "Processed 1400 users\n"
     ]
    }
   ],
   "source": [
    "subs_list = []\n",
    "groups_freq_dict = {}\n",
    "top_n = 5\n",
    "\n",
    "for record, uid in zip(final_data, uids):\n",
    "    try:\n",
    "        user_subs = record\n",
    "        if not user_subs.get('response'):\n",
    "            user_subs = vk_get_response(\n",
    "                'users.getSubscriptions', 'user_id={0}&v=4.9&lang=ru'.format(int(uid)), access_token\n",
    "            )\n",
    "        subs_pd = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    'groups_count': user_subs['response']['groups'].get('count'),\n",
    "                    'groups_list': user_subs['response']['groups'].get('items'),\n",
    "                    'follows_count':user_subs['response']['users'].get('count'),\n",
    "                    'follows_list': user_subs['response']['users'].get('items'),\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for group_id in user_subs['response']['groups'].get('items')[:top_n]:\n",
    "            if groups_freq_dict.get(group_id):\n",
    "                groups_freq_dict[group_id] += 1\n",
    "            else:\n",
    "                groups_freq_dict[group_id] = 1\n",
    "\n",
    "        subs_pd['subs_count'] = subs_pd['groups_count'] + subs_pd['follows_count']\n",
    "        subs_list.append(subs_pd)\n",
    "    except:\n",
    "#         print(user_subs)\n",
    "        pass\n",
    "    if len(subs_list) % 100 == 0:\n",
    "        print(\"Processed {0} users\".format(len(subs_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_subs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самые популярные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(key, val) for key, val in groups_freq_dict.items()], key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка постов со стен групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 groups extracted\n",
      "200 groups extracted\n",
      "300 groups extracted\n",
      "400 groups extracted\n",
      "500 groups extracted\n",
      "600 groups extracted\n",
      "700 groups extracted\n",
      "800 groups extracted\n",
      "900 groups extracted\n",
      "1000 groups extracted\n",
      "1100 groups extracted\n",
      "1200 groups extracted\n",
      "1300 groups extracted\n",
      "1400 groups extracted\n",
      "1500 groups extracted\n",
      "1600 groups extracted\n",
      "1700 groups extracted\n",
      "Response error. Group id 163822605\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-163822605'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "1800 groups extracted\n",
      "1900 groups extracted\n",
      "2000 groups extracted\n",
      "2100 groups extracted\n",
      "2200 groups extracted\n",
      "2300 groups extracted\n",
      "2400 groups extracted\n",
      "2500 groups extracted\n",
      "2600 groups extracted\n",
      "2700 groups extracted\n",
      "2800 groups extracted\n",
      "Response error. Group id 18379957\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-18379957'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "2900 groups extracted\n",
      "Response error. Group id 1493020\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-1493020'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "3000 groups extracted\n",
      "3100 groups extracted\n",
      "3200 groups extracted\n",
      "3300 groups extracted\n",
      "3400 groups extracted\n",
      "3500 groups extracted\n",
      "3600 groups extracted\n",
      "3700 groups extracted\n",
      "3800 groups extracted\n",
      "3900 groups extracted\n",
      "4000 groups extracted\n",
      "4100 groups extracted\n",
      "Response error. Group id 121318641\n",
      "{'error': {'error_code': 15, 'error_msg': 'Access denied: this wall available only for community members', 'request_params': [{'key': 'oauth', 'value': '1'}, {'key': 'method', 'value': 'wall.get'}, {'key': 'owner_id', 'value': '-121318641'}, {'key': 'count', 'value': '100'}, {'key': 'fields', 'value': 'post_type,marked_as_ads'}, {'key': '', 'value': ''}, {'key': 'v', 'value': '4.9'}, {'key': 'lang', 'value': 'ru'}]}}\n",
      "4200 groups extracted\n",
      "4300 groups extracted\n",
      "4400 groups extracted\n"
     ]
    }
   ],
   "source": [
    "group_doc_dict = {}\n",
    "counter = 0\n",
    "groups_freq_dict_top5 = groups_freq_dict\n",
    "check={}\n",
    "\n",
    "for group_id, freq in groups_freq_dict_top5.items():\n",
    "    counter += 1\n",
    "    try:\n",
    "        check = vk_get_response(\n",
    "            'wall.get',\n",
    "            'owner_id={0}&count=100&fields=post_type,marked_as_ads&&v=4.9&lang=ru'.format(int(group_id) * -1),\n",
    "            access_token\n",
    "        )\n",
    "        check = check['response']\n",
    "        group_doc = ''\n",
    "        if check[0] != 0:\n",
    "            for post in check[1:]:\n",
    "                if post.get('marked_as_ads') != 1:\n",
    "                    group_doc += post['text']\n",
    "        group_doc_dict[group_id] = group_doc\n",
    "    except:\n",
    "        print(\"Response error. Group id {0}\".format(group_id))\n",
    "        print(check)\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} groups extracted\".format(counter))\n",
    "    time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить сырые данные по постам групп на диск\n",
    "save_obj(group_doc_dict, 'group_doc_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_doc_dict=load_obj('group_doc_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vndan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vndan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "#!pip install nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#!pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs_to_delete = string.punctuation + u'»' + u'«' + u'—' + u'“' + u'„' + u'•' + u'#'\n",
    "translation_table = {ord(c): None for c in chrs_to_delete if c != u'-'}\n",
    "units = MorphAnalyzer.DEFAULT_UNITS\n",
    "morph = MorphAnalyzer(result_type=None, units=units)\n",
    "PortSt = PorterStemmer()\n",
    "stopw = set(\n",
    "    [w for w in stopwords.words(['russian', 'english'])]\n",
    "    + [u'это', u'году', u'года', u'также', u'етот',\n",
    "       u'которые', u'который', u'которая', u'поэтому',\n",
    "       u'весь', u'свой', u'мочь', u'eтот', u'например',\n",
    "       u'какой-то', u'кто-то', u'самый', u'очень', u'несколько',\n",
    "       u'источник', u'стать', u'время', u'пока', u'однако',\n",
    "       u'около', u'немного', u'кроме', u'гораздо', u'каждый',\n",
    "       u'первый', u'вполне', u'из-за', u'из-под',\n",
    "       u'второй', u'нужно', u'нужный', u'просто', u'большой',\n",
    "       u'хороший', u'хотеть', u'начать', u'должный', u'новый', u'день',\n",
    "       u'метр', u'получить', u'далее', u'именно', u'апрель',\n",
    "       u'сообщать', u'разный', u'говорить', u'делать',\n",
    "       u'появиться', u'2016',\n",
    "       u'2015', u'получить', u'иметь', u'составить', u'дать', u'читать',\n",
    "       u'ничто', u'достаточно', u'использовать',\n",
    "       u'принять', u'практически',\n",
    "       u'находиться', u'месяц', u'достаточно', u'что-то', u'часто',\n",
    "       u'хотеть', u'начаться', u'делать', u'событие', u'составлять',\n",
    "       u'остаться', u'заявить', u'сделать', u'дело',\n",
    "       u'примерно', u'попасть', u'хотя', u'лишь', u'первое',\n",
    "       u'больший', u'решить', u'число', u'идти', u'давать', u'вопрос',\n",
    "       u'сегодня', u'часть', u'высокий', u'главный', u'случай', u'место',\n",
    "       u'конец', u'работать', u'работа', u'слово', u'важный', u'сказать']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_start = 'http[s]?://'\n",
    "url_end = (\n",
    "    '(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    ")\n",
    "pattern = url_start + url_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка слов постов групп - трансформация в \"хороший\" вид. Нормализация и стэмминг, удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 docs processed\n",
      "200 docs processed\n",
      "300 docs processed\n",
      "400 docs processed\n",
      "500 docs processed\n",
      "600 docs processed\n",
      "700 docs processed\n",
      "800 docs processed\n",
      "900 docs processed\n",
      "1000 docs processed\n",
      "1100 docs processed\n",
      "1200 docs processed\n",
      "1300 docs processed\n",
      "1400 docs processed\n",
      "1500 docs processed\n",
      "1600 docs processed\n",
      "1700 docs processed\n",
      "1800 docs processed\n",
      "1900 docs processed\n",
      "2000 docs processed\n",
      "2100 docs processed\n",
      "2200 docs processed\n",
      "2300 docs processed\n",
      "2400 docs processed\n",
      "2500 docs processed\n",
      "2600 docs processed\n",
      "2700 docs processed\n",
      "2800 docs processed\n",
      "2900 docs processed\n",
      "3000 docs processed\n",
      "3100 docs processed\n",
      "3200 docs processed\n",
      "3300 docs processed\n",
      "3400 docs processed\n",
      "3500 docs processed\n",
      "3600 docs processed\n",
      "3700 docs processed\n",
      "3800 docs processed\n",
      "3900 docs processed\n",
      "4000 docs processed\n",
      "4100 docs processed\n",
      "4200 docs processed\n",
      "4300 docs processed\n",
      "4400 docs processed\n"
     ]
    }
   ],
   "source": [
    "group_clean_doc_dict = {}\n",
    "counter = 0\n",
    "\n",
    "for group_id, doc in group_doc_dict.items():\n",
    "    soup = BeautifulSoup(doc, 'html.parser')\n",
    "    body = ' '.join(\n",
    "        [tag.string.replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "         for tag in soup.descendants if tag.string]\n",
    "    )\n",
    "    body = re.sub('\\[.*?\\]','', body)\n",
    "    body = re.sub(pattern,'', body)\n",
    "    if body != '':\n",
    "        body_clean = body.translate(translation_table).lower().strip()\n",
    "        words = word_tokenize(body_clean)\n",
    "        tokens = []\n",
    "        # stemming and text normalization\n",
    "        for word in words:\n",
    "            if re.match('^[a-z0-9-]+$', word) is not None:\n",
    "                tokens.append(PortSt.stem(word))\n",
    "            elif word.count('-') > 1:\n",
    "                tokens.append(word)\n",
    "            else:\n",
    "                normal_forms = morph.normal_forms(word)\n",
    "                tokens.append(normal_forms[0] if normal_forms else word)\n",
    "        # remove stopwords and leave unique words only\n",
    "        tokens = filter(\n",
    "            lambda token: token not in stopw, sorted(set(tokens))\n",
    "        )\n",
    "\n",
    "        # remove all words with more than 3 chars\n",
    "        tokens = filter(lambda token: len(token) > 3, tokens)\n",
    "    else:\n",
    "        tokens = []\n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        print(\"{0} docs processed\".format(counter))\n",
    "    group_clean_doc_dict[group_id] = tokens\n",
    "\n",
    "group_clean_doc_dict = {key: list(val) for key, val in group_clean_doc_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохранить обработанные данные на диск\n",
    "save_obj(group_clean_doc_dict, 'group_doc_dict_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_clean_doc_dict=load_obj('group_doc_dict_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'лечение'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_clean_doc_dict[163316424][666]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение LDA модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in d:\\program files\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\program files\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in d:\\program files\\anaconda3\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\program files\\anaconda3\\lib\\site-packages (from gensim) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\program files\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto3 in d:\\program files\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (1.9.111)\n",
      "Requirement already satisfied: requests in d:\\program files\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.19.1)\n",
      "Requirement already satisfied: boto>=2.32 in d:\\program files\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: bz2file in d:\\program files\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in d:\\program files\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.111 in d:\\program files\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.111)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in d:\\program files\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in d:\\program files\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.23)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in d:\\program files\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\program files\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\program files\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in d:\\program files\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in d:\\program files\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.111->boto3->smart-open>=1.7.0->gensim) (0.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "#import gensim\n",
    "from gensim.corpora import TextCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "class ListTextCorpus(TextCorpus):\n",
    "\n",
    "    def get_texts(self):\n",
    "        for doc in self.input:\n",
    "            yield doc\n",
    "                \n",
    "mycorp = ListTextCorpus(input=group_clean_doc_dict.values())\n",
    "justlda = LdaModel(\n",
    "    corpus=mycorp, num_topics=20, passes=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel performance\n",
      "0 береть мирачёрный завтракдышать отпустили…ледить будущего❤️американский псовнеадекватный люди💙американский чувствололитакавказский 2❤️счастливый бёрдвокруг\n",
      "1 девичьепремьер dimvit поспелов i8bmw durand fatum bonni mckee utrecht asot900\n",
      "2 болс керека жақс үшін сатып жакс алдын мена болада немес\n",
      "3 darknet прочитанного🔥🔥🔥 ахуеесть шығарып тіпті өтініш жігіт мықт қиналып ертең\n",
      "4 ингредиент приготовление масло соль сахара рецепт вкусный смесь сахар ложка\n",
      "5 😍норвегиянорвегиясить yеllowston ρarkшвейцариябарселонаперумароккошвейцариядоминиканашвейцарияпарижбагамынорвегиягавайигрецияшвейцарияамстердамчехияиталиянью-йорккрымгальштатавстрияфлоренциясахарашвейцарияиталияшвейцарияавстриянорвегияпортугалиякраска ✨красиво ✨гренландиялондоншвейцарияфилиппиныанглиянорвегияшвейцарияшвейцариябудапештбайкалкрыманглиясингапуршвейцариярумынияамстердамвашингтоннидерландыавстралияшвейцарияшвейцариябайкалкитайчехиябрюггеальпыпрагачилиболгарияканадагавайиэверестанглиякрасивоканадашвейцариян 🌲италиябудапештшвейцарияиспанияперудомбайалтайгейрангерпаганчехияфудзипраганорвегиялихтенштайнпортофинокрасиво 🌲🌲🌲канадарек обьиталиянорвегиябайкалкрымлуганоамстердамбаварияперуиталия знать жизнь\n",
      "6 настрoение манyть кoить дейcтвительный плoха рeбятка pубитый дикo дeдушка дивaный\n",
      "7 яббаров саленко ларченко африкантовый безусый дмитренко кадонь мондезира блюменкранец друзьяк\n",
      "8 тoлькo cвoть чтoб кoгдa кoтopый мoжeт пocлe caмый дeнь этoт\n",
      "9 биoгpaфия гepoeть coбcтвeннoгo плaнeтe зaмecтитeль cтpaннoe штopм дoкyмeнтaльный мyзыкoть зaглядывaeт\n",
      "10 caмoм coбoть бoльшe нecкoлькo вaшa cкoлькo cтpaнa ceгoдень вceть poccие\n",
      "11 ребенкавсё работун другэтый человеккот безопасностит мнойпомылсяпросто жекто большаякогда нимибодрить старостинеожиданноочередьпошло-поехалоотличный\n",
      "12 любить жизнь человек жить знать твой любовь друг видеть думать\n",
      "13 государство политический власть государственный война правительство глава военный войско президент\n",
      "14 март человек группа новое город друг россия ждать последний жизнь\n",
      "15 реал манчестер юнайтед тоттенхэма челси пенальти динамо ювентус ливерпуль забивать\n",
      "16 hardcor void corsten trax ✅никакий почте⚠ ✅укреплять ⚠оплата pearl bandit\n",
      "17 иcкусство тpанспорт непрекращающегoся александрафотограф шуваловафотограф endegorфотограф анастасияфотограф arharфотограф сорокинфотограф мизиновафотограф\n",
      "18 ⏰2230 ⏰2030 ⏰1630 ⏰1900 ⏰1705 ⏰1500 ⌚2030 ⌚2300 ⏰2000 🇮🇹серия\n",
      "19 love music live time black life hous world make record\n"
     ]
    }
   ],
   "source": [
    "print('LdaModel performance')\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group distribution by the most relevant topic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14    0.60\n",
       "12    0.29\n",
       "4     0.05\n",
       "13    0.02\n",
       "0     0.02\n",
       "19    0.01\n",
       "8     0.00\n",
       "2     0.00\n",
       "10    0.00\n",
       "16    0.00\n",
       "15    0.00\n",
       "6     0.00\n",
       "1     0.00\n",
       "3     0.00\n",
       "7     0.00\n",
       "11    0.00\n",
       "18    0.00\n",
       "5     0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dict = {key: 0 for key in range(20)}\n",
    "\n",
    "group_topics_dict_20 = {\n",
    "    group_id: dict(list(dummy_dict.items()) + justlda.get_document_topics(mycorp.dictionary.doc2bow(text)))\n",
    "    for group_id, text in group_clean_doc_dict.items()\n",
    "}\n",
    "check_pd_20 = pd.DataFrame.from_dict(group_topics_dict_20, orient='index')\n",
    "check_pd_20.head(10)\n",
    "print(\"Group distribution by the most relevant topic\")\n",
    "pd.Series.round(check_pd_20.idxmax(axis=1).value_counts() * 1. / len(check_pd_20), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump lda model to disk\n",
    "justlda.save('ldamodel_20_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most typical groups for every topic\n",
      "0 береть мирачёрный завтракдышать отпустили…ледить будущего❤️американский псовнеадекватный люди💙американский чувствололитакавказский 2❤️счастливый бёрдвокруг\n",
      "я знаю этот фильм http://vk.com/club162599423\n",
      "ЧЁТ ОРНУЛ http://vk.com/club124080209\n",
      "Бандерівський легіон http://vk.com/club23987113\n",
      "● secret hideaway http://vk.com/club41663122\n",
      "Почитатели японской анимации и манги http://vk.com/club14129120\n",
      "E s t h é t i q u e http://vk.com/club34238786\n",
      "Приятная жесть http://vk.com/club70596213\n",
      "мальчики твоего двора http://vk.com/club145824554\n",
      "Анонимус http://vk.com/club24117813\n",
      "Аристократичка http://vk.com/club138586391\n",
      "\n",
      "1 девичьепремьер dimvit поспелов i8bmw durand fatum bonni mckee utrecht asot900\n",
      "BMW http://vk.com/club15721446\n",
      "Just™ http://vk.com/club17316055\n",
      "Арт Бот http://vk.com/club147720339\n",
      "Лучшее видео (18+) http://vk.com/club23208414\n",
      "Новые песни 2019 http://vk.com/club32936794\n",
      "Мультфильмы и фильмы торрент http://vk.com/club41080902\n",
      "jlyb http://vk.com/club19220411\n",
      "Депресенин http://vk.com/club108557167\n",
      "Короче говоря http://vk.com/club144495454\n",
      "Бешеный юмор http://vk.com/club65802500\n",
      "\n",
      "2 болс керека жақс үшін сатып жакс алдын мена болада немес\n",
      "Признавашки ТОРТКУЛЬ!!! http://vk.com/club101089780\n",
      "Оңашадағы Онлайн Ойлар (ООО) http://vk.com/club126055085\n",
      "Moto Brothers http://vk.com/club35212833\n",
      "ТҮРЛІ ОҚИҒАЛАР http://vk.com/club112410559\n",
      "∞ GGG ∞ http://vk.com/club96913262\n",
      "°ӘЛЕМ КӘСІПҚОЙ БОКСЫ° http://vk.com/club74316114\n",
      "♡`shanel shop `ss`♡ http://vk.com/club149035507\n",
      "Жұлдызды әлем http://vk.com/club71558031\n",
      "GUP ® http://vk.com/club89571518\n",
      "ЖАҢАӨЗЕН АЛТЫН КҮМІС...ƏШЕКЕЙ БҰЙЫМДАРЫ... http://vk.com/club139223438\n",
      "\n",
      "3 darknet прочитанного🔥🔥🔥 ахуеесть шығарып тіпті өтініш жігіт мықт қиналып ертең\n",
      "Cosy Hearth http://vk.com/club79320083\n",
      "\"Жігіт Қыз\"  Танысу Бекеті http://vk.com/club152671252\n",
      "причины моего психического расстройства http://vk.com/club154168174\n",
      "ЖАҢАӨЗЕН АЛТЫН КҮМІС...ƏШЕКЕЙ БҰЙЫМДАРЫ... http://vk.com/club139223438\n",
      "Андроид | Android http://vk.com/club27680436\n",
      "Малогабаритка | Идеи для маленьких квартир http://vk.com/club154606397\n",
      "6 сынып жиынтық бағалау http://vk.com/club172711100\n",
      "ТҮРЛІ ОҚИҒАЛАР http://vk.com/club112410559\n",
      "Дизайн Интерьера | 33 кв. метра http://vk.com/club153272708\n",
      "Оңашадағы Онлайн Ойлар (ООО) http://vk.com/club126055085\n",
      "\n",
      "4 ингредиент приготовление масло соль сахара рецепт вкусный смесь сахар ложка\n",
      "Рецепты. Книга ИДЕАЛЬНОЙ ХОЗЯЙКИ http://vk.com/club52334003\n",
      "Ложка - вкусные рецепты http://vk.com/club172149046\n",
      "Энциклопедия хозяйки|Рецепты.Кулинарные хитрости http://vk.com/club60971453\n",
      "Здоровая Кухня  - рецепты / правильное питание http://vk.com/club154607382\n",
      "Диетические рецепты http://vk.com/club34889014\n",
      "Рецепты от шеф-повара | Кулинар http://vk.com/club43551797\n",
      "Готовим дома: вкусно и просто http://vk.com/club40020627\n",
      "Быстрые рецепты http://vk.com/club165062392\n",
      "Cook Good - лучшие рецепты http://vk.com/club39009769\n",
      "Счастливая хозяйка http://vk.com/club178284678\n",
      "\n",
      "5 😍норвегиянорвегиясить yеllowston ρarkшвейцариябарселонаперумароккошвейцариядоминиканашвейцарияпарижбагамынорвегиягавайигрецияшвейцарияамстердамчехияиталиянью-йорккрымгальштатавстрияфлоренциясахарашвейцарияиталияшвейцарияавстриянорвегияпортугалиякраска ✨красиво ✨гренландиялондоншвейцарияфилиппиныанглиянорвегияшвейцарияшвейцариябудапештбайкалкрыманглиясингапуршвейцариярумынияамстердамвашингтоннидерландыавстралияшвейцарияшвейцариябайкалкитайчехиябрюггеальпыпрагачилиболгарияканадагавайиэверестанглиякрасивоканадашвейцариян 🌲италиябудапештшвейцарияиспанияперудомбайалтайгейрангерпаганчехияфудзипраганорвегиялихтенштайнпортофинокрасиво 🌲🌲🌲канадарек обьиталиянорвегиябайкалкрымлуганоамстердамбаварияперуиталия знать жизнь\n",
      "Красивые места планеты: туризм http://vk.com/club111155620\n",
      "Клуб Злобных Атеистов http://vk.com/club58536821\n",
      "Почитатели японской анимации и манги http://vk.com/club14129120\n",
      "● secret hideaway http://vk.com/club41663122\n",
      "Безумные приколы http://vk.com/club30751873\n",
      "Бандерівський легіон http://vk.com/club23987113\n",
      "Я фотошоплю как бог http://vk.com/club75338985\n",
      "Хорошо сказано http://vk.com/club34152189\n",
      "ОСАГО|КАСКО|ТЕХОСМОТР ЕКАТЕРИНБУРГ http://vk.com/club171821883\n",
      "Пример виджета Текст http://vk.com/club166155543\n",
      "\n",
      "6 настрoение манyть кoить дейcтвительный плoха рeбятка pубитый дикo дeдушка дивaный\n",
      "ты сохранишь http://vk.com/club49468741\n",
      "ПРИКОЛЫ | Смеяка http://vk.com/club45441631\n",
      "Улетные приколы http://vk.com/club22741624\n",
      "МХК http://vk.com/club33414947\n",
      "Бумажный самолётик http://vk.com/club52537634\n",
      "Злой Гений http://vk.com/club39153701\n",
      "Строки из песен http://vk.com/club150372412\n",
      "Создано Природой http://vk.com/club155777378\n",
      "jlyb http://vk.com/club19220411\n",
      "Депресенин http://vk.com/club108557167\n",
      "\n",
      "7 яббаров саленко ларченко африкантовый безусый дмитренко кадонь мондезира блюменкранец друзьяк\n",
      "Фильмы онлайн новинки  Кино 2019 http://vk.com/club40150367\n",
      "Кот, что по ночам орёт http://vk.com/club143815310\n",
      "Сеткилимден сенээ http://vk.com/club27380192\n",
      "Анекдоты | Сарказм http://vk.com/club67308144\n",
      "♥ЫНАКШЫЛ ШУЛУКТЕРИ♥17RUS http://vk.com/club33775324\n",
      "Частырыгны Кижи Кылыр (чажыт) за 30-40-50 http://vk.com/club76178172\n",
      "Boduun_Setkil http://vk.com/club167432806\n",
      "ТУВИНСКИЕ ОТКРОВЕНИЯ | ТЫВА | ТУВА http://vk.com/club50945454\n",
      "SWAG GIRLS http://vk.com/club24527053\n",
      "Сочные GIF 18+ http://vk.com/club152751710\n",
      "\n",
      "8 тoлькo cвoть чтoб кoгдa кoтopый мoжeт пocлe caмый дeнь этoт\n",
      "HD Фильмы | Новинки кино 2019 http://vk.com/club52548908\n",
      "Я ❤ КИНО http://vk.com/club113071474\n",
      "HD Кино - Фильмы онлайн 2019 http://vk.com/club123915905\n",
      "НОВИНКИ КИНО 2019 http://vk.com/club90253744\n",
      "Фильмы / Кино /Капитан Marvel Алина 2019 http://vk.com/club73595075\n",
      "МИР КИНО http://vk.com/club287912\n",
      "Police GIF | Полиция http://vk.com/club149019396\n",
      "ФИЛЬМЫ / КИНО  2019 | НОВИНКИ ОНЛАЙН http://vk.com/club35457671\n",
      "Play Films - фильмы Full HD http://vk.com/club40870056\n",
      "Циник http://vk.com/club26090632\n",
      "\n",
      "9 биoгpaфия гepoeть coбcтвeннoгo плaнeтe зaмecтитeль cтpaннoe штopм дoкyмeнтaльный мyзыкoть зaглядывaeт\n",
      "НОВИНКИ КИНО 2019 http://vk.com/club90253744\n",
      "HD Кино - Фильмы онлайн 2019 http://vk.com/club123915905\n",
      "КИНО В ХОРОШЕМ КАЧЕСТВЕ http://vk.com/club74868793\n",
      "Безумные приколы http://vk.com/club107740696\n",
      "РАШКА - КВАДРАТНЫЙ ВАТНИК (официальный паблик) http://vk.com/club88487497\n",
      "Не переплачивай http://vk.com/club172986969\n",
      "AliExpress для радиолюбителя http://vk.com/club172623740\n",
      "Запчасти для бытовой техники. http://vk.com/club177170795\n",
      "(удалено) http://vk.com/club1715902\n",
      "Empire History http://vk.com/club34940333\n",
      "\n",
      "10 caмoм coбoть бoльшe нecкoлькo вaшa cкoлькo cтpaнa ceгoдень вceть poccие\n",
      "Лепрозорий http://vk.com/club65960786\n",
      "Академия Настоящих Бойцов | UFC | БОКС http://vk.com/club138649060\n",
      "✿Яңа җырлар һәм котлаулар✿ ТАТАРСТАН| КАЗАНЬ-УФА http://vk.com/club70660094\n",
      "Не поверишь! http://vk.com/club28477986\n",
      "Мужской рай http://vk.com/club38290762\n",
      "I ❤️ART http://vk.com/club23626127\n",
      "Курск http://vk.com/club99926754\n",
      "Тонкий юмор http://vk.com/club25679656\n",
      "ВКазани Поймут | Главное сообщество Казани http://vk.com/club57867786\n",
      "Город Орёл | Орловчане ВКонтакте http://vk.com/club2197800\n",
      "\n",
      "11 ребенкавсё работун другэтый человеккот безопасностит мнойпомылсяпросто жекто большаякогда нимибодрить старостинеожиданноочередьпошло-поехалоотличный\n",
      "Четкие Приколы http://vk.com/club23064236\n",
      "ЛЕЩ 53 http://vk.com/club177112082\n",
      "Empire History http://vk.com/club34940333\n",
      "Клуб Злобных Атеистов http://vk.com/club58536821\n",
      "Почитатели японской анимации и манги http://vk.com/club14129120\n",
      "Бандерівський легіон http://vk.com/club23987113\n",
      "Безумные приколы http://vk.com/club30751873\n",
      "Я фотошоплю как бог http://vk.com/club75338985\n",
      "ОСАГО|КАСКО|ТЕХОСМОТР ЕКАТЕРИНБУРГ http://vk.com/club171821883\n",
      "Хорошо сказано http://vk.com/club34152189\n",
      "\n",
      "12 любить жизнь человек жить знать твой любовь друг видеть думать\n",
      "Sin Psychology http://vk.com/club51347431\n",
      "True Pleasure &amp; Грани любви http://vk.com/club63426952\n",
      "СТИХИ КРИТИКА GDG http://vk.com/club8849029\n",
      "Turdus merula http://vk.com/club81419424\n",
      "Тишина | Ирина Карапетян | Стихи http://vk.com/club61313933\n",
      "как в книге написано http://vk.com/club150933653\n",
      "Ты сильная. Унесешь. http://vk.com/club47138504\n",
      "я тебя люблю, я тебя тоже нет http://vk.com/club42355584\n",
      "Musical datings http://vk.com/club30555165\n",
      "Анекдоты http://vk.com/club127068613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 государство политический власть государственный война правительство глава военный войско президент\n",
      "Український Гумор http://vk.com/club62731197\n",
      "Р А С Е Я Н С Т В О http://vk.com/club43901086\n",
      "Легіон УСС \"Історичні меми до ЗНО\" http://vk.com/club148363520\n",
      "Сакральные тексты http://vk.com/club111677185\n",
      "Канал 1+1 http://vk.com/club6970317\n",
      "Ольга Богомолець http://vk.com/club5199881\n",
      "Ультрас України | UKRAINE ULTRAS http://vk.com/club4823978\n",
      "Андрей Фурсов рекомендует http://vk.com/club13992166\n",
      "Типичный Киев http://vk.com/club32195333\n",
      "Типова Україна http://vk.com/club34691663\n",
      "\n",
      "14 март человек группа новое город друг россия ждать последний жизнь\n",
      "Marquiz | Конструктор квизов для маркетинга http://vk.com/club153335049\n",
      "Работа в Москве http://vk.com/club12248221\n",
      "Райффайзенбанк Россия http://vk.com/club76875894\n",
      "ВКонтакте для бизнеса http://vk.com/club19542789\n",
      "Школа №73 г. Тольятти http://vk.com/club4340022\n",
      "Grintern.ru http://vk.com/club42496831\n",
      "КОТЕЛЬНИКИ.ИНФО / НЕЗАВИСИМАЯ ОТКРЫТАЯ площадка http://vk.com/club64832603\n",
      "Авторынок. Нижний Новгород http://vk.com/club124405175\n",
      "ВРЕМЯ МЕСТНОЕ. Железногорск (Красноярский край) http://vk.com/club61965058\n",
      "Гвардейское http://vk.com/club34898896\n",
      "\n",
      "15 реал манчестер юнайтед тоттенхэма челси пенальти динамо ювентус ливерпуль забивать\n",
      "- ADV CLUB | Авто, Мото, Тюнинг http://vk.com/club9494209\n",
      "ФУТБОЛ: №1 http://vk.com/club38077047\n",
      "ФУТБОЛ: Лига Чемпионов http://vk.com/club25450736\n",
      "Мир Футбола http://vk.com/club68139409\n",
      "Футбол Европы http://vk.com/club23693281\n",
      "Футбольные обзоры |  Новости футбола http://vk.com/club35027856\n",
      "Мировой Футбол | Лига Чемпионов http://vk.com/club4290276\n",
      "Чё, футболист, да? Красавчик! |  ФУТБОЛ http://vk.com/club28374702\n",
      "Реальный Футбол http://vk.com/club71474813\n",
      "Европейский Футбол | Ювентус - Атлетико http://vk.com/club12637219\n",
      "\n",
      "16 hardcor void corsten trax ✅никакий почте⚠ ✅укреплять ⚠оплата pearl bandit\n",
      "Эту страну не победить! http://vk.com/club4097\n",
      "Иң шәп татарча җырлар | Татарстан| Казань| Челны http://vk.com/club29532414\n",
      "E X C I T E http://vk.com/club173271915\n",
      "Приколись 18+ http://vk.com/club112230375\n",
      "Пошлые Приколы 18+ http://vk.com/club149640858\n",
      "Слова Великих людей http://vk.com/club28981879\n",
      "Орленок http://vk.com/club36775802\n",
      "ЛайфХАК - своими руками http://vk.com/club49690338\n",
      "Интертат: иң популяр татар сайты http://vk.com/club30044977\n",
      "✿Яңа җырлар һәм котлаулар✿ ТАТАРСТАН| КАЗАНЬ-УФА http://vk.com/club70660094\n",
      "\n",
      "17 иcкусство тpанспорт непрекращающегoся александрафотограф шуваловафотограф endegorфотограф анастасияфотограф arharфотограф сорокинфотограф мизиновафотограф\n",
      "ФОТОГРАФЫ | УРОКИ | ИДЕИ | ФОТОШОП http://vk.com/club160903\n",
      "Ecstasy http://vk.com/club136255956\n",
      "Строки из песен http://vk.com/club150372412\n",
      "Бешеный юмор http://vk.com/club65802500\n",
      "Няшки - Милашки .  Красивые  девушки . http://vk.com/club65484616\n",
      "Коты и кошки http://vk.com/club34578663\n",
      "Мультфильмы и фильмы торрент http://vk.com/club41080902\n",
      "jlyb http://vk.com/club19220411\n",
      "Депресенин http://vk.com/club108557167\n",
      "Короче говоря http://vk.com/club144495454\n",
      "\n",
      "18 ⏰2230 ⏰2030 ⏰1630 ⏰1900 ⏰1705 ⏰1500 ⌚2030 ⌚2300 ⏰2000 🇮🇹серия\n",
      "BMW | Mercedes | Audi http://vk.com/club34812270\n",
      "👑Audi™ http://vk.com/club88194369\n",
      "Мир Футбола http://vk.com/club68139409\n",
      "I'm a Footballer | ФУТБОЛ http://vk.com/club26544717\n",
      "Футбольные обзоры |  Новости футбола http://vk.com/club35027856\n",
      "ФУТБОЛ: №1 http://vk.com/club38077047\n",
      "ФУТБОЛ: Лига Чемпионов http://vk.com/club25450736\n",
      "Чё, футболист, да? Красавчик! |  ФУТБОЛ http://vk.com/club28374702\n",
      "Champions Cup | ФУТБОЛ http://vk.com/club8722610\n",
      "Ябвдул 18+ http://vk.com/club50630084\n",
      "\n",
      "19 love music live time black life hous world make record\n",
      "Pro Evolution Soccer | Faces http://vk.com/club47881762\n",
      "how to use http://vk.com/club8516234\n",
      "Armada Music http://vk.com/club43195089\n",
      "Hardwell http://vk.com/club56074769\n",
      "Armin van Buuren http://vk.com/club42743690\n",
      "lmao http://vk.com/club163058008\n",
      "$$$ DANK MEMES $$$ AYY LMAO $$$ http://vk.com/club120254617\n",
      "Bananastreet — клубная музыка http://vk.com/club183079\n",
      "Paul Van Dyk http://vk.com/club42467072\n",
      "Enrique Iglesias http://vk.com/club46583541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The most typical groups for every topic\")\n",
    "for i in range(20):\n",
    "    terms = justlda.get_topic_terms(i)\n",
    "    print(i, ' '.join(map(lambda x: mycorp.dictionary.get(x[0]), terms)))\n",
    "    typical_groups = check_pd_20[i].sort_values(ascending=False).index[:10]\n",
    "    for g in typical_groups:\n",
    "        group_info = vk_get_response(\n",
    "            'groups.getById', 'group_ids={0}&v=4.9&lang=ru'.format(g), access_token\n",
    "        )\n",
    "        print(group_info['response'][0]['name'] + ' ' + 'http://vk.com/club' + str(g))\n",
    "        time.sleep(0.3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#понять бы что получилось перед тем как проанализировать аудитории))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
